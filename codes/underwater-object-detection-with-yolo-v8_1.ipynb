{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the images and labels directories\n",
    "train_images = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/train/images'\n",
    "train_labels = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/train/labels'\n",
    "\n",
    "val_images = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/valid/images'\n",
    "val_labels = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/valid/labels'\n",
    "\n",
    "test_images = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/test/images'\n",
    "test_labels = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/test/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the yaml data file\n",
    "yaml_path = 'C:/Users/Shahid Ahamed Hasib/Downloads/archive/data.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image has dimensions 1024x768 and 3 channels\n"
     ]
    }
   ],
   "source": [
    "# Read a image by path\n",
    "image_path = os.path.join(train_images, os.listdir(train_images)[100])\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Get the size of the image\n",
    "height, width, channels = image.shape\n",
    "print('The image has dimensions {}x{} and {} channels'.format(height, width, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to Label Mapping: {0: 'fish', 1: 'jellyfish', 2: 'penguin', 3: 'puffin', 4: 'shark', 5: 'starfish', 6: 'stingray'}\n",
      "Label to Index Mapping: {'fish': 0, 'jellyfish': 1, 'penguin': 2, 'puffin': 3, 'shark': 4, 'starfish': 5, 'stingray': 6}\n"
     ]
    }
   ],
   "source": [
    "# Define the labels\n",
    "classes = ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n",
    "Idx2Label = {idx: label for idx, label in enumerate(classes)}\n",
    "Label2Index = {label: idx for idx, label in Idx2Label.items()}\n",
    "print('Index to Label Mapping:', Idx2Label)\n",
    "print('Label to Index Mapping:', Label2Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved as 'output_image_grid.png'\n"
     ]
    }
   ],
   "source": [
    "def visualize_image_with_annotation_bboxes(image_dir, label_dir):\n",
    "    # Get list of all the image files in the directory\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    \n",
    "    # Choose 10 random image files from the list\n",
    "    sample_image_files = random.sample(image_files, 12)\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    \n",
    "    # Loop over the random images and plot the bounding boxes\n",
    "    for i, image_file in enumerate(sample_image_files):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        # Load the image\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load the labels for this image\n",
    "        label_path = os.path.join(label_dir, image_file[:-4] + '.txt')\n",
    "        f = open(label_path, 'r')\n",
    "        \n",
    "        # Loop over the labels and plot the bounding boxes\n",
    "        for label in f:\n",
    "            class_id, x_center, y_center, width, height = map(float, label.split())\n",
    "            h, w, _ = image.shape\n",
    "            x_min = int((x_center - width/2) * w)\n",
    "            y_min = int((y_center - height/2) * h)\n",
    "            x_max = int((x_center + width/2) * w)\n",
    "            y_max = int((y_center + height/2) * h)\n",
    "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            cv2.putText(image, Idx2Label[int(class_id)], (x_min, y_min), cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 255, 255), thickness=2)\n",
    "    \n",
    "        axs[row, col].imshow(image)\n",
    "        axs[row, col].axis('off')\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig('output_image_grid.png')  # Save the figure as an image file\n",
    "    plt.close()  # Close the plot to avoid placeholder output\n",
    "    print(\"Figure saved as 'output_image_grid.png'\")\n",
    "\n",
    "# Visualize 12 sample images with bounding boxes\n",
    "visualize_image_with_annotation_bboxes(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.is_available()\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.11 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.10  Python-3.12.3 torch-2.4.1+cpu \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid CUDA 'device=gpu' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: 0\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# You can use 'yolov8s.pt', 'yolov8m.pt', etc. for different sizes\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43myaml_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to the dataset YAML file\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                 \u001b[49m\u001b[38;5;66;43;03m# Number of training epochs (adjust as needed)\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                 \u001b[49m\u001b[38;5;66;43;03m# Image size for training (can be adjusted)\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# Initial learning rate\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlrf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                   \u001b[49m\u001b[38;5;66;43;03m# Final learning rate (fraction of initial LR)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                   \u001b[49m\u001b[38;5;66;43;03m# Batch size (adjust based on your CPU/GPU memory)\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                              \u001b[49m\u001b[38;5;66;43;03m# Using 'cpu' since CUDA is not available\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munderwater_detection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Folder name for saving the model and results\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Extract the mAP score\u001b[39;00m\n\u001b[0;32m     18\u001b[0m maP_50 \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics/mAP50(B)\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;66;03m# mAP@0.5 score\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shahid Ahamed Hasib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:796\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    794\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Users\\Shahid Ahamed Hasib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:103\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(cfg, overrides)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_resume(overrides)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shahid Ahamed Hasib\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:192\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[0;32m    185\u001b[0m         LOGGER\u001b[38;5;241m.\u001b[39minfo(s)\n\u001b[0;32m    186\u001b[0m         install \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA devices are seen by torch.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[1;32m--> 192\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requested.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or pass valid CUDA device(s) if available,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m i.e. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice=0,1,2,3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for Multi-GPU.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtorch.cuda.is_available(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtorch.cuda.device_count(): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mos.environ[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvisible\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mps \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():  \u001b[38;5;66;03m# prefer GPU if available\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     devices \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# i.e. \"0,1\" -> [\"0\", \"1\"]\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid CUDA 'device=gpu' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: 0\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"
     ]
    }
   ],
   "source": [
    "# Load a YOLOv8 model, for example, the nano version for fast training\n",
    "model = YOLO('yolov8n.pt')  # You can use 'yolov8s.pt', 'yolov8m.pt', etc. for different sizes\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data= yaml_path ,  # Path to the dataset YAML file\n",
    "    epochs=1,                                 # Number of training epochs (adjust as needed)\n",
    "    imgsz=(height, width, channels),                                 # Image size for training (can be adjusted)\n",
    "    lr0=0.1,                                  # Initial learning rate\n",
    "    lrf=0.01,                                   # Final learning rate (fraction of initial LR)\n",
    "    seed=42,\n",
    "    batch=8,                                   # Batch size (adjust based on your CPU/GPU memory)\n",
    "    workers=4,\n",
    "    device='gpu',                              # Using 'cpu' since CUDA is not available\n",
    "    name='underwater_detection'                # Folder name for saving the model and results\n",
    ")\n",
    "\n",
    "# Extract the mAP score\n",
    "maP_50 = results.metrics['metrics/mAP50(B)']   # mAP@0.5 score\n",
    "print(f\"mAP@0.5: {maP_50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# read in the results.csv file as a pandas dataframe\n",
    "df = pd.read_csv('path.csv')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# create subplots using seaborn\n",
    "fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\n",
    "\n",
    "# plot the columns using seaborn\n",
    "sns.lineplot(x='epoch', y='train/box_loss', data=df, ax=axs[0,0])\n",
    "sns.lineplot(x='epoch', y='train/cls_loss', data=df, ax=axs[0,1])\n",
    "sns.lineplot(x='epoch', y='train/dfl_loss', data=df, ax=axs[1,0])\n",
    "sns.lineplot(x='epoch', y='metrics/precision(B)', data=df, ax=axs[1,1])\n",
    "sns.lineplot(x='epoch', y='metrics/recall(B)', data=df, ax=axs[2,0])\n",
    "sns.lineplot(x='epoch', y='metrics/mAP50(B)', data=df, ax=axs[2,1])\n",
    "sns.lineplot(x='epoch', y='metrics/mAP50-95(B)', data=df, ax=axs[3,0])\n",
    "sns.lineplot(x='epoch', y='val/box_loss', data=df, ax=axs[3,1])\n",
    "sns.lineplot(x='epoch', y='val/cls_loss', data=df, ax=axs[4,0])\n",
    "sns.lineplot(x='epoch', y='val/dfl_loss', data=df, ax=axs[4,1])\n",
    "\n",
    "# set titles and axis labels for each subplot\n",
    "axs[0,0].set(title='Train Box Loss')\n",
    "axs[0,1].set(title='Train Class Loss')\n",
    "axs[1,0].set(title='Train DFL Loss')\n",
    "axs[1,1].set(title='Metrics Precision (B)')\n",
    "axs[2,0].set(title='Metrics Recall (B)')\n",
    "axs[2,1].set(title='Metrics mAP50 (B)')\n",
    "axs[3,0].set(title='Metrics mAP50-95 (B)')\n",
    "axs[3,1].set(title='Validation Box Loss')\n",
    "axs[4,0].set(title='Validation Class Loss')\n",
    "axs[4,1].set(title='Validation DFL Loss')\n",
    "\n",
    "# add suptitle and subheader\n",
    "plt.suptitle('Training Metrics and Loss', fontsize=24)\n",
    "\n",
    "# adjust top margin to make space for suptitle\n",
    "plt.subplots_adjust(top=0.8)\n",
    "\n",
    "# adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best performing model\n",
    "model = YOLO('path/runs/detect/yolov8n_custom/weights/best.pt')\n",
    "\n",
    "# Evaluating the model on test dataset\n",
    "metrics = model.val(conf=0.25, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform detections with trained model\n",
    "def predict_detection(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Pass the image through the detection model and get the result\n",
    "    detect_result = model(image)\n",
    "    \n",
    "    # Plot the detections\n",
    "    detect_image = detect_result[0].plot()\n",
    "    \n",
    "    # Convert the image to RGB format\n",
    "    detect_image = cv2.cvtColor(detect_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return detect_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all the image files in the test directory\n",
    "image_files = sorted(os.listdir(test_images))\n",
    "    \n",
    "# Choose 12 random image files from the list\n",
    "sample_image_files = random.sample(image_files, 12)\n",
    "    \n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    \n",
    "# Loop over the random images and plot the detections of the trained model\n",
    "for i, image_file in enumerate(sample_image_files):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "        \n",
    "    # Load the current image and run object detection\n",
    "    image_path = os.path.join(test_images, image_file)\n",
    "    detect_image = predict_detection(image_path)\n",
    "    \n",
    "    axs[row, col].imshow(detect_image)\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
